
---
title: "Introduction to Classification Methods"
author: "Rebecca C. Steorts, Duke University "
date: STA 325, Chapter 4 ISL
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Agenda
===
- Classification revisited
- Issues with linear regression
- Linear discrimminant analysis
- Lab with tasks to complete for homework 

Classification
===
Classification is a predictive task in which the response takes
values across discrete categories (i.e., not continuous), and in the 
most fundamental case, two categories.

\bigskip
Examples:
\begin{itemize}
\item Predicting whether a patient will develop breast cancer or remain
healthy, given genetic information
\item Predicting whether or not a user will like a new product, based on
user covariates and a history of his/her previous ratings
\item Predicting the region of Italy in which a brand of olive oil was 
made, based on its chemical composition
\item Predicting the next elected president, based on various social,
political, and historical measurements
\end{itemize}

Classification
===
The point of classification methods is to accurately assign
new, unlabeled examples, from the test data} to these classes.  

This is "supervised" learning because we can check the performance on the labeled
training data.  

The point of calculating information was to select features which made classification easier.

Classification versus Clustering
===
Similar to our usual setup, we observe pairs $(x_i,y_i)$, $i=1,\ldots n$, 
where $y_i$ gives the class of the $i$th observation, and $x_i \in \R^p$ are
the measurements of $p$ predictor variables

\bigskip
Though the class labels may actually be $y_i \in \{\mathrm{healthy}, 
\mathrm{sick}\}$ or $y_i \in \{\mathrm{Sardinia}, \mathrm{Sicily}, ... \}$, 
but we can always encode them as 
$$y_i \in \{1,2,\ldots K\}$$
where $K$ is the total number of classes

\bigskip
Why is classification different from clustering? 

- In classification there is not a pre-defined notion of class 
membership (and sometimes, not even $K$). 

- We are not given the response variable $y_i$ but only $x_i$, $i=1,\ldots n$
(meaning we have no labeled data).

Classification versus clustering
===
Constructed from training data $(x_i,y_i)$, $i=1,\ldots n$, we denote our 
classification rule by $\hat{f}(x)$; given any $x \in \R^p$, this returns a 
class label $\hat{f}(x) \in \{1,\ldots K\}$

\bigskip
As before, we will see that there are two different ways of assessing the quality of $\hat{f}$: its predictive ability and interpretative ability

Binary classification and linear regression
===
Let's start off by supposing that $K=2$, so that the response is 
$y_i \in \{1,2\}$, for $i=1,\ldots n$

\bigskip
You already know a tool that you could potentially use in this case for classification: linear regression. 

Simply treat the response as if it were continuous, and find the linear regression coefficients of the response vector $y \in \R^n$ onto the predictors, i.e., 

$$\hbeta_0, \hbeta = \argmin_{\beta_0\in\R, \,\beta \in \R^p} \, 
\sum_{i=1}^n (y_i - \beta_0 - x_i^T \beta)^2 $$

Then, given a new input $x_0 \in \R^p$, we predict the class to be
$$\hat{f}^\mathrm{LS}(x_0) = \begin{cases}
1 & \mathrm{if}\; \hbeta_0 + x_0^T \hbeta \leq 1.5 \\
2 & \mathrm{if}\; \hbeta_0 + x_0^T \hbeta  > 1.5
\end{cases}$$

Linear regression continued for classification
===
(Note: since we included an intercept term in the regression, it doesn't
matter whether we code the class labels as $\{1,2\}$ or $\{0,1\}$, etc.)

\bigskip
In many instances, this actually works reasonably well. Examples:

\bigskip
\includegraphics[width=0.33\textwidth]{lin1.pdf}
\includegraphics[width=0.33\textwidth]{lin2.pdf}
\includegraphics[width=0.33\textwidth]{lin3.pdf}

\bigskip
Overall, using linear regression in this way for binary classification is not 
a crazy idea. But how about if there are more than 2 classes?

Linear regression (K>2)
===
Given $K$ classes, define the indicator matrix $Y \in \R^{n\times K}$ to be the matrix whose columns indicate class membership.  

That is, its $j$th column satisfies $Y_{ij} = 1$ if $y_i=j$ (observation $i$ is in class $j$) and $Y_{ij}=0$ otherwise.

\bigskip
E.g., with $n=6$ observations and $K=3$ classes, the matrix 
$$Y = 
\left(\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1 \\
\end{array}\right) \in \R^{6 \times 3}$$
corresponds to having the first two observations in class 1, the next two
in class 2, and the final 2 in class 3

Linear regression of indicators
===
To construct a prediction rule, we regress 
each column $Y_j \in \R^n$ (indicating the $j$th class versus all else)
onto the predictors:
$$\hbeta_{j,0}, \hbeta_j = \argmin_{\beta_{j,0}\in\R,\,\beta_j \in \R^p} 
\, \sum_{i=1}^n (Y_{ij} - \beta_{0,j} - \beta_j^T x_i)^2$$

Now, given a new input $x_0 \in \R^p$, we compute
$$\hbeta_{0,j} + x_0^T \hbeta_j, \;\;\; j=1,\ldots K$$
take predict the class $j$ that corresponds to the highest score. I.e., we let
each of the $K$ linear models make its own prediction, and then we take 
the strongest. Formally, $$\hat{f}^\mathrm{LS}(x_0) = 
\argmax_{j=1,\ldots K} \; \hbeta_{0,j} + x_0^T \hbeta_j $$

Linear regression for classification
===
The decision boundary between any two classes $j,k$ are the values of
$x \in \R^p$ for which 
$$\hbeta_{0,j} + x^T \hbeta_j  = \hbeta_{0,k} + x^T \hbeta_k$$
i.e., $\hbeta_{0,j}-\hbeta_{0,k} + (\hbeta_j-\hbeta_k)^T x = 0$

\bigskip
\bigskip
\begin{tabular}{cc}
\parbox{0.475\textwidth}{
\includegraphics[width=0.475\textwidth]{blank.png}}
%\includegraphics[width=0.475\textwidth]{db.png}}
\hspace{-15pt}
\parbox{0.5\textwidth}{
This defines a $(p-1)$-dimensional affine subspace in $\R^p$. 
% it's actually affine if we include intercept
To one side, we would always predict class $j$
over $k$; to the other, we would always predict class $k$ over $j$}
\end{tabular}

\bigskip
\bigskip
\bigskip
For $K$ classes total, there are ${K \choose 2}=\frac{K(K-1)}{2}$ decision boundaries

Linear regression for classification
===
What we'd like to see when we use linear regression for a
3-way classification (from ESL page 105):
\vspace{-2pt}
\begin{center}
\includegraphics[width=2.25in]{ideal.png}
\end{center}
\vspace{-5pt}
The plotted lines are the decision boundaries between classes 1 and 2, and 2 and 3
(the decision boundary between classes 1 and 3 never matters)

Linear regression for classification
===
\smallskip
What actually happens when we use linear regression for this 3-way classification
(from ESL page 105):
\begin{center}
\includegraphics[width=2.20in]{actual.png}
\end{center}
\vspace{-5pt}
The decision boundaries between 1 and 2 and between 2 and 3 are the same, so we 
would never predict class 2. This problem is called  masking (and it is not
uncommon for moderate $K$ and small $p$)


Statistical Decision Theory
===
Let $C$ be a random variable giving the class label of an 
observation in our data set. A natural rule would be to classify 
according to 
$$f(x) = \argmax_{j=1,\ldots K} \; \P(C=j|X=x)$$
This predicts the most likely class, given the feature measurements $X=x \in \R^p$.
This is called the Bayes classifier, and it is the best that we can do (think 
of overlapping classes)

\bigskip
Note that we can use Bayes' rule to write
$$\P(C=j|X=x) = \frac{\P(X=x|C=j) \cdot \P(C=j)}
{\P(X=x)}$$
Let $\pi_j = \P(C=j)$ be the prior probability of class $j$.
Since the Bayes classifier compares the above quantity across $j=1,\ldots K$ 
for $X=x$, the denominator is always the same, hence
$$f(x) = \argmax_{j=1,\ldots K}\; \P(X=x|C=j) \cdot \pi_j$$

Linear discrimminant analysis
===
Using the Bayes classifier is not realistic as it requires knowing the 
class conditional densities $\P(X=x|C=j)$ and prior probabilities $\pi_j$. 
But if estimate these quantities, then we can follow the idea

\bigskip
Linear discriminant analysis (LDA) does this by assuming that 
the data within each class are normally distributed: 
$$h_j(x) = \P(X=x|C=j) = N(\mu_j, \Sigma) \;\mathrm{density} $$
We allow each class to have its own mean $\mu_j \in \R^p$, but 
we assume a common covariance matrix $\Sigma \in \R^{p \times p}$.
Hence
$$h_j(x) = \frac{1}{(2\pi)^{p/2} \mathrm{det}(\Sigma)^{1/2}}
\exp\Big\{-\frac{1}{2}(x-\mu_j)^T \Sigma^{-1} (x-\mu_j)\Big\}$$
So we want to find $j$ so that $\P(C=j|X=x)\cdot\pi_j =
h_j(x)\cdot\pi_j$ is the largest

LDA (continued)
===
Since $\log(\cdot)$ is a monotone function, we can consider maximizing
$\log(h_j(x) \pi_j)$ over $j=1,\ldots K$. We can define the rule:
\begin{align*}
f^\mathrm{LDA}(x) \;\;\;&=
\argmax_{j=1,\ldots K} \; \delta_j(x) 
\end{align*} 

\bigskip
We call $\delta_j(x)$, $j=1,\ldots K$ the discriminant functions. Note
$$\delta_j(x) = x^T \Sigma^{-1} \mu_j - \frac{1}{2}\mu_j^T \Sigma^{-1} 
\mu_j + \log\pi_j $$
is just an affine function of $x$

LDA (continued)
===
In practice, given an input $x\in\R^p$, can we just use the rule 
$f^\mathrm{LDA}$ on the previous slide?
Not quite! What's missing: we don't know $\pi_j$, $\mu_j$, and $\Sigma$.

Therefore, 
we estimate them based on the training data $x_i \in \R^p$ and 
$y_i \in \{1,\ldots K\}$, $i=1,\ldots n$, by:

\begin{itemize}
\item $\hat{\pi}_j = n_j/n$, the proportion of observations in class $j$
\item $\hat{\mu}_j = \frac{1}{n_j} \sum_{y_i=j} x_i$, the centroid of class $j$
\item $\hat{\Sigma} = \frac{1}{n-K} \sum_{j=1}^K \sum_{y_i=j} 
(x_i - \hat{\mu}_j)(x_i - \hat{\mu}_j)^T$, the pooled sample covariance matrix
\end{itemize}
(Here $n_j$ is the number of points in class $j$)

LDA (continued)
===
We replace $\pi_j,\mu_j,\Sigma$ by their sample estimates based 
on labeled observations $y_i \in \{1,\ldots K\}$, $x_i \in \R^p$, $i=1,\ldots n$,
\begin{gather*}
\hat{\pi}_j = n_j/n, \;\;\; \hat{\mu}_j = \frac{1}{n_j}\sum_{y_i=j} x_i, \\
\hat{\Sigma} = \frac{1}{n-K} \sum_{j=1}^K \sum_{y_i=j} (x_i - \hat{\mu}_j)
(x_i - \hat{\mu}_j)^T
\end{gather*}

\bigskip
This gives the estimated discriminant functions:
$$\hat{\delta}_j(x) = x^T \hat{\Sigma}^{-1} \hat{\mu}_j - 
\frac{1}{2}\hat{\mu}_j^T \hat{\Sigma}^{-1}\hat{\mu}_j + \log\hat{\pi}_j$$
and finally the linear discriminant analysis rule,
$$\hat{f}^\mathrm{LDA}(x) = \argmax_{j=1,\ldots K} \; \hat{\delta}_j(x)$$

LDA Decision Boundaries
===
The estimated discriminant functions
\begin{align*}
\hat{\delta}_j(x) &= x^T \hat{\Sigma}^{-1} \hat{\mu}_j - 
\frac{1}{2}\hat{\mu}_j^T \hat{\Sigma}^{-1}\hat{\mu}_j + \log\hat{\pi}_j \\
&= a_j + b_j^T x
\end{align*}
are just affine functions of $x$. 
The decision boundary between classes $j,k$ is the set of all 
$x \in \R^p$ such that $\hat{\delta}_j(x)=\hat{\delta}_k(x)$, i.e.,
$$a_j + b_j^T x = a_k + b_k^T x$$

\bigskip
\begin{tabular}{cc}
\parbox{0.45\textwidth}{
This defines an affine subspace in $x$:
$$ a_j-a_k + (b_j-b_k)^T x = 0 $$} &
\parbox{0.4\textwidth}{
\includegraphics[width=0.5\textwidth]{blank.png}}
\end{tabular}

Example: LDA Decision boundaries
===
Example of decision boundaries from LDA (from ESL page 109):
\begin{center}
$f^\mathrm{LDA}(x)$ \hspace{1.5in} $\hat{f}^\mathrm{LDA}(x)$ \\
\smallskip
\includegraphics[width=4in]{ldaex.png}
\end{center} 
Are the decision boundaries the same as the perpendicular 
bisectors between the class centroids?
(Why not?)

LDA usage and extensions
===
The decision boundaries for LDA are useful for graphical 
purposes, but to classify a new point $x_0 \in \R^p$ we don't
use them---we simply compute $\hat{\delta}_j(x_0)$ for each 
$j=1,\ldots K$

\bigskip
LDA performs quite well on a wide variety of data sets, even when
pitted against fancy alternative classification schemes. Though
it assumes normality, its simplicity often works in its favor.
(Why? Think of the bias-variance tradeoff)

\bigskip
Still, there are some useful extensions of LDA. E.g.,
\begin{itemize}
\item Quadratic discriminant analysis: using the same normal model,
we now allow each class $j$ to have its own covariance matrix $\Sigma_j$.
This leads to quadratic decision boundaries
\item Reduced-rank linear discriminant analysis: we essentially 
project the data to a lower dimensional subspace before performing LDA.
\end{itemize}

Application to Weekly Data
===
We're going to work with the Weekly data set that is part of the ISLR package. 

It contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. 

Task 1
===

Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

Solution to Task 1
===
\tiny
```{r}
library(ISLR)
data(Weekly)
names(Weekly)
summary(Weekly)
attach(Weekly)
```

Solution to Task 1
===
```{r}
plot(Volume)
```

Yes, there is a pattern of increasing volume over time.

Task 2
===
Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

Solution to Task 2
===
```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 +
        Lag5 + Volume, family="binomial", data=Weekly)
summary(glm.fit)
```

From the analysis above, Lag 2 is statistically significant with a p-value of 0.03. The postive coefficient for this predictor suggests that if the market had a positive return yesterday, then it is more likely to go up today. Since the p-value is small, the is clear evidence of an association between Lag1 and Direction.

Task 3
===
Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\footnote{A confusion matrix is explained on p. 145 of the James et. al (2013) text. See Table 4.5.} Calculate the false positive and false negative rates here.

Solution to Task 3
===
Recall back in the LSH module we looked at classifications as being:

1. Pairs of data can be linked in both the handmatched training
data (which we refer to as ‘truth’) and under the estimated
linked data. We refer to this situation as true positives (TP).
2. Pairs of data can be linked under the truth but not linked
under the estimate, which are called false negatives (FN).
3. Pairs of data can be not linked under the truth but linked
under the estimate, which are called false positives (FP).
4. Pairs of data can be not linked under the truth and also not
linked under the estimate, which we refer to as true negatives
(TN).

\begin{table}[htdp]
\caption{Confusion Matrix on Full Data for Direction}
\begin{center}
\begin{tabular}{c|ccc}
  Pred (Estimated)
 &  &      True   & False \\ \hline 
Truth &   True  &  True positive & False negative \\ 
      &   False   &  False positive & True negative\end{tabular}
\end{center}
\label{default}
\end{table}



Solution to Task 3
===
You can now calculate the FNR and FPR using the following formulas: 

 $$FPR = \frac{ \text{false positives}}
{(\text{false positives} + \text{true negatives})}.$$ Also, $$FNR =  \frac{\text{false negatives}}{(\text{false negatives} + \text{true positives})}.$$

Give the confusion matrix and calculate the FNR and FPR. 

Task 4
===

Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). Is there an improvement and if so in what sense? 
(Think about how the false positive and false negative rates have changed).

Task 5
===
Repeat Task 4 using LDA. 

Task 6
=== 

Repeat Task 4 using QDA. Which method is better: logistic regression, LDA, or QDA. Make sure you can defend your answer. 




